{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as th \n",
    "from torch.nn import MSELoss\n",
    "from torch_geometric.data import Data, Batch\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "from model import Generator, Discriminator, gradient_penalty, polygon_penalty\n",
    "from utils import Transformed_PolyGraphDataset, CATEGORY_DICT\n",
    "\n",
    "import numpy as np\n",
    "import time "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Hyperparameters\n",
    "\"\"\"\n",
    "\n",
    "MAX_POLYGONS = 30       # max nr. of polygons to output\n",
    "\n",
    "# Optimizer params\n",
    "g_lr = 0.001 \n",
    "d_lr = 0.001\n",
    "b1 = 0.5 \n",
    "b2 = 0.999  \n",
    "\n",
    "# WGAN params\n",
    "N_critic = 5            # nr of times to train discriminator more\n",
    "lambda_gp = 10          # gradient penalty hyperpraram\n",
    "\n",
    "# Training params\n",
    "MAX_EPOCHS = 50\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Network parameters\n",
    "NOISE_SIZE = 128\n",
    "HIDDEN_GENERATOR = [64, 32, 32]                 # list of dimensions for hidden layers\n",
    "OUTPUT_GENERATOR = MAX_POLYGONS * 2             # we want to output at most this many polygons per node (note [x1... y1...] format)\n",
    "\n",
    "HIDDEN_DISCRIMINATOR = [64, 32, 16]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model definition\n",
    "\n",
    "### Generator\n",
    "- Input of Generator will always be: \"noise size + Nr. of categories\" \n",
    "- Output of Generator = (MAX_POYLGONS * 2) due to our output coordinate format\n",
    "\n",
    "\n",
    "### Discriminator\n",
    "- Input of Discriminator = output of Generator (MAX_POLYGONS * 2) due to our output coordinate format\n",
    "- Output of Discriminator will always be 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModuleList(\n",
      "  (0): TAGConv(141, 64, K=3)\n",
      "  (1): TAGConv(64, 32, K=3)\n",
      "  (2): TAGConv(32, 60, K=3)\n",
      ")\n",
      "ModuleList(\n",
      "  (0): TAGConv(60, 64, K=3)\n",
      "  (1): TAGConv(64, 32, K=3)\n",
      "  (2): TAGConv(32, 16, K=3)\n",
      "  (3): Linear(in_features=16, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\"\"\" \n",
    "Model Definitions\n",
    "\"\"\"\n",
    "generator = Generator(input_dim=NOISE_SIZE + len(CATEGORY_DICT), \n",
    "                      output_dim=OUTPUT_GENERATOR, \n",
    "                      hidden_dims=HIDDEN_GENERATOR)\n",
    "\n",
    "discriminator = Discriminator(input_dim=OUTPUT_GENERATOR, \n",
    "                              hidden_dims=HIDDEN_DISCRIMINATOR)\n",
    "\n",
    "print(generator.module_list)\n",
    "print(discriminator.module_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizers\n",
    "optimizer_G = th.optim.Adam(generator.parameters(), lr=g_lr, betas=(b1, b2)) \n",
    "optimizer_D = th.optim.Adam(discriminator.parameters(), lr=d_lr, betas=(b1, b2))\n",
    "\n",
    "# Loss instantiation\n",
    "mse_loss = MSELoss()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training loop\n",
    "\n",
    "Generator details:\n",
    "- We generate a noise vector\n",
    "- Generator uses noise vector and training data (the categories) as input\n",
    "- Generator uses graph NN layers to generate coordinates out of the noise vectors using the graph structure and categories\n",
    "- Generator output: (MAX_POLYGONS * 2) for each node in the input\n",
    "\n",
    "Discriminator details:\n",
    "- Generate discriminator output (score) for real data and fake (generated) data\n",
    "- Compute loss over these scores with additional gradient penalty loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(generator, discriminator, optimizer_g, optimizer_d, data_loader):\n",
    "\n",
    "    losses_g, losses_d = [], []\n",
    "    num_steps = 0\n",
    "\n",
    "    # real = batch of Data() ex. [Data(), Data(), ...] is 1 batch\n",
    "    start_time = time.time()\n",
    "    for i, real in enumerate(data_loader):\n",
    "        num_steps += 1\n",
    "\n",
    "        # Input noise_data into generator            \n",
    "        noise = th.randn((len(real.category), NOISE_SIZE))\n",
    "        fake = generator(real, noise)     \n",
    "\n",
    "        # fake.shape = (batch_size * nodes, output_features = 60)\n",
    "        # We must turn this into appropriate (batch) input for the discriminator\n",
    "        fake = Batch(geometry=fake, edge_index=real.edge_index, batch=real.batch)\n",
    "\n",
    "        discriminator_fake = discriminator(fake)    # discriminator scores for fakes\n",
    "        discriminator_real = discriminator(real)    # discriminator scores for reals\n",
    "        \n",
    "        gp = gradient_penalty(discriminator, real, fake)\n",
    "\n",
    "        # Discriminator loss and train\n",
    "        loss_discriminator = -(th.mean(discriminator_real) - th.mean(discriminator_fake)) + lambda_gp * gp\n",
    "        \n",
    "        discriminator.zero_grad() \n",
    "        loss_discriminator.backward() \n",
    "        optimizer_d.step()\n",
    "\n",
    "        losses_d.append(loss_discriminator.item())\n",
    "\n",
    "        # Only train Generator every 5 steps\n",
    "        if num_steps % N_critic == 0:\n",
    "            noise_g = th.randn((len(real.category), NOISE_SIZE))\n",
    "            fake_g = generator(real, noise_g) \n",
    "            fake_g = Batch(geometry=fake_g, edge_index=real.edge_index, batch=real.batch)\n",
    "\n",
    "            output = discriminator(fake_g).reshape(-1)      # discriminator scores for fake\n",
    "            # loss for genereator = the discriminators' judgement\n",
    "            # higher score = better so lower loss\n",
    "            loss_generator = -th.mean(output) + mse_loss(real.geometry, fake_g.geometry)\n",
    "            generator.zero_grad()\n",
    "            loss_generator.backward()\n",
    "            optimizer_g.step()\n",
    "\n",
    "            losses_g.append(loss_generator.item())\n",
    "        \n",
    "        # 49 is just a temporary number\n",
    "        if i % 50 == 0:\n",
    "            batch_time = time.time() - start_time\n",
    "            print(\"Batch {} took {:.2f} seconds.\".format(i, batch_time))\n",
    "    \n",
    "    return sum(losses_g) / len(losses_g), sum(losses_d) / len(losses_d)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main training loop\n",
    "\n",
    "Code could be improved. Make sure to specify your own 'generator_path', 'discriminator_path'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main training loop\n",
    "def train(generator, discriminator, optimizer_g, optimizer_d, data_loader, max_epochs):\n",
    "    start_time = time.time() \n",
    "    losses_g, losses_d = [], []\n",
    "\n",
    "    for epoch in range(1, max_epochs + 1):\n",
    "        epoch_time = time.time() \n",
    "\n",
    "        loss_g, loss_d = run_epoch(generator, discriminator, optimizer_g, optimizer_d, data_loader)\n",
    "        losses_g.append(loss_g)\n",
    "        losses_d.append(loss_d)\n",
    "\n",
    "        # TODO: Evaluation and logging code??\n",
    "        epoch_time = time.time() - epoch_time \n",
    "        total_time = time.time() - start_time\n",
    "        print(\"Total runtime: {:.2f}, Epoch: {}, took: {:.2f} seconds, loss_g: {:.2f}, loss_d: {:.2f}\".format(total_time, epoch, epoch_time, loss_g, loss_d))\n",
    "\n",
    "        # save model every 10 epochs\n",
    "        if epoch % 10:\n",
    "            print(\"Saving model at epoch {}\".format(epoch))\n",
    "            generator_path = r'C:\\School\\DELFT\\Graph_ML_project\\saved_models\\generator.pt'\n",
    "            discriminator_path = r'C:\\School\\DELFT\\Graph_ML_project\\saved_models\\discriminator.pt'\n",
    "            th.save(generator.state_dict(), generator_path)\n",
    "            th.save(discriminator.state_dict(), discriminator_path)\n",
    "\n",
    "    return losses_g, losses_d"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "294\n",
      "Batch 0 took 0.17 seconds.\n",
      "Batch 50 took 9.83 seconds.\n",
      "Batch 100 took 19.40 seconds.\n",
      "Batch 150 took 28.67 seconds.\n",
      "Batch 200 took 37.88 seconds.\n",
      "Batch 250 took 47.29 seconds.\n",
      "Total runtime: 55.95, Epoch: 1, took: 55.95 seconds, loss_g: -441.47, loss_d: -1973.15\n",
      "Saving model at epoch 1\n",
      "Batch 0 took 0.20 seconds.\n",
      "Batch 50 took 10.84 seconds.\n",
      "Batch 100 took 22.06 seconds.\n",
      "Batch 150 took 31.74 seconds.\n",
      "Batch 200 took 41.14 seconds.\n",
      "Batch 250 took 51.06 seconds.\n",
      "Total runtime: 115.54, Epoch: 2, took: 59.57 seconds, loss_g: -110.30, loss_d: -1647.77\n",
      "Saving model at epoch 2\n",
      "Batch 0 took 0.21 seconds.\n",
      "Batch 50 took 9.79 seconds.\n",
      "Batch 100 took 19.94 seconds.\n",
      "Batch 150 took 29.83 seconds.\n",
      "Batch 200 took 39.44 seconds.\n",
      "Batch 250 took 49.19 seconds.\n",
      "Total runtime: 172.84, Epoch: 3, took: 57.30 seconds, loss_g: -327.57, loss_d: -1719.19\n",
      "Saving model at epoch 3\n",
      "Batch 0 took 0.17 seconds.\n",
      "Batch 50 took 9.79 seconds.\n",
      "Batch 100 took 19.56 seconds.\n",
      "Batch 150 took 29.81 seconds.\n",
      "Batch 200 took 39.34 seconds.\n",
      "Batch 250 took 49.03 seconds.\n",
      "Total runtime: 231.44, Epoch: 4, took: 58.60 seconds, loss_g: -423.10, loss_d: -1427.36\n",
      "Saving model at epoch 4\n",
      "Batch 0 took 0.22 seconds.\n",
      "Batch 50 took 10.18 seconds.\n",
      "Batch 100 took 20.15 seconds.\n",
      "Batch 150 took 30.10 seconds.\n",
      "Batch 200 took 40.31 seconds.\n",
      "Batch 250 took 49.73 seconds.\n",
      "Total runtime: 288.93, Epoch: 5, took: 57.48 seconds, loss_g: -17.80, loss_d: -1338.63\n",
      "Saving model at epoch 5\n",
      "Batch 0 took 0.20 seconds.\n",
      "Batch 50 took 9.98 seconds.\n",
      "Batch 100 took 19.17 seconds.\n",
      "Batch 150 took 28.48 seconds.\n",
      "Batch 200 took 38.17 seconds.\n",
      "Batch 250 took 47.22 seconds.\n",
      "Total runtime: 344.01, Epoch: 6, took: 55.07 seconds, loss_g: 141.24, loss_d: -1189.08\n",
      "Saving model at epoch 6\n",
      "Batch 0 took 0.18 seconds.\n",
      "Batch 50 took 9.96 seconds.\n",
      "Batch 100 took 18.84 seconds.\n",
      "Batch 150 took 28.13 seconds.\n",
      "Batch 200 took 37.19 seconds.\n",
      "Batch 250 took 45.99 seconds.\n",
      "Total runtime: 397.64, Epoch: 7, took: 53.63 seconds, loss_g: 412.32, loss_d: -982.31\n",
      "Saving model at epoch 7\n",
      "Batch 0 took 0.16 seconds.\n",
      "Batch 50 took 9.31 seconds.\n",
      "Batch 100 took 18.26 seconds.\n",
      "Batch 150 took 27.22 seconds.\n",
      "Batch 200 took 36.57 seconds.\n",
      "Batch 250 took 45.69 seconds.\n",
      "Total runtime: 451.09, Epoch: 8, took: 53.45 seconds, loss_g: 177.61, loss_d: -779.37\n",
      "Saving model at epoch 8\n",
      "Batch 0 took 0.18 seconds.\n",
      "Batch 50 took 9.38 seconds.\n",
      "Batch 100 took 18.42 seconds.\n",
      "Batch 150 took 27.87 seconds.\n",
      "Batch 200 took 36.87 seconds.\n",
      "Batch 250 took 46.95 seconds.\n",
      "Total runtime: 506.09, Epoch: 9, took: 55.00 seconds, loss_g: -11.37, loss_d: -656.50\n",
      "Saving model at epoch 9\n",
      "Batch 0 took 0.17 seconds.\n",
      "Batch 50 took 9.48 seconds.\n",
      "Batch 100 took 18.93 seconds.\n",
      "Batch 150 took 28.60 seconds.\n",
      "Batch 200 took 38.47 seconds.\n",
      "Batch 250 took 48.32 seconds.\n",
      "Total runtime: 561.90, Epoch: 10, took: 55.80 seconds, loss_g: -129.00, loss_d: -596.06\n",
      "Batch 0 took 0.17 seconds.\n",
      "Batch 50 took 8.81 seconds.\n",
      "Batch 100 took 17.57 seconds.\n",
      "Batch 150 took 26.27 seconds.\n",
      "Batch 200 took 35.03 seconds.\n",
      "Batch 250 took 44.05 seconds.\n",
      "Total runtime: 613.38, Epoch: 11, took: 51.48 seconds, loss_g: -179.15, loss_d: -529.15\n",
      "Saving model at epoch 11\n",
      "Batch 0 took 0.17 seconds.\n",
      "Batch 50 took 8.62 seconds.\n",
      "Batch 100 took 17.35 seconds.\n",
      "Batch 150 took 26.53 seconds.\n",
      "Batch 200 took 35.39 seconds.\n",
      "Batch 250 took 44.16 seconds.\n",
      "Total runtime: 665.07, Epoch: 12, took: 51.69 seconds, loss_g: -319.30, loss_d: -512.25\n",
      "Saving model at epoch 12\n",
      "Batch 0 took 0.18 seconds.\n",
      "Batch 50 took 8.94 seconds.\n",
      "Batch 100 took 17.63 seconds.\n",
      "Batch 150 took 26.44 seconds.\n",
      "Batch 200 took 35.17 seconds.\n",
      "Batch 250 took 44.44 seconds.\n",
      "Total runtime: 717.64, Epoch: 13, took: 52.56 seconds, loss_g: -384.53, loss_d: -481.95\n",
      "Saving model at epoch 13\n",
      "Batch 0 took 0.15 seconds.\n",
      "Batch 50 took 10.50 seconds.\n",
      "Batch 100 took 21.58 seconds.\n",
      "Batch 150 took 30.69 seconds.\n",
      "Batch 200 took 39.77 seconds.\n",
      "Batch 250 took 48.49 seconds.\n",
      "Total runtime: 774.29, Epoch: 14, took: 56.65 seconds, loss_g: -392.42, loss_d: -472.10\n",
      "Saving model at epoch 14\n",
      "Batch 0 took 0.18 seconds.\n",
      "Batch 50 took 9.66 seconds.\n",
      "Batch 100 took 18.82 seconds.\n",
      "Batch 150 took 28.10 seconds.\n",
      "Batch 200 took 37.00 seconds.\n",
      "Batch 250 took 46.75 seconds.\n",
      "Total runtime: 829.27, Epoch: 15, took: 54.97 seconds, loss_g: -366.47, loss_d: -466.07\n",
      "Saving model at epoch 15\n",
      "Batch 0 took 0.18 seconds.\n",
      "Batch 50 took 9.87 seconds.\n",
      "Batch 100 took 19.05 seconds.\n",
      "Batch 150 took 28.43 seconds.\n",
      "Batch 200 took 37.73 seconds.\n",
      "Batch 250 took 46.90 seconds.\n",
      "Total runtime: 884.03, Epoch: 16, took: 54.76 seconds, loss_g: -530.73, loss_d: -481.34\n",
      "Saving model at epoch 16\n",
      "Batch 0 took 0.18 seconds.\n",
      "Batch 50 took 10.06 seconds.\n",
      "Batch 100 took 19.72 seconds.\n",
      "Batch 150 took 29.58 seconds.\n",
      "Batch 200 took 40.12 seconds.\n",
      "Batch 250 took 49.60 seconds.\n",
      "Total runtime: 941.41, Epoch: 17, took: 57.37 seconds, loss_g: -798.79, loss_d: -503.72\n",
      "Saving model at epoch 17\n",
      "Batch 0 took 0.15 seconds.\n",
      "Batch 50 took 9.28 seconds.\n",
      "Batch 100 took 18.32 seconds.\n",
      "Batch 150 took 27.81 seconds.\n",
      "Batch 200 took 37.98 seconds.\n",
      "Batch 250 took 47.07 seconds.\n",
      "Total runtime: 996.21, Epoch: 18, took: 54.80 seconds, loss_g: -1177.70, loss_d: -608.01\n",
      "Saving model at epoch 18\n",
      "Batch 0 took 0.16 seconds.\n",
      "Batch 50 took 9.40 seconds.\n",
      "Batch 100 took 18.78 seconds.\n",
      "Batch 150 took 27.75 seconds.\n",
      "Batch 200 took 37.19 seconds.\n",
      "Batch 250 took 46.79 seconds.\n",
      "Total runtime: 1050.93, Epoch: 19, took: 54.71 seconds, loss_g: -1765.22, loss_d: -680.64\n",
      "Saving model at epoch 19\n",
      "Batch 0 took 0.16 seconds.\n",
      "Batch 50 took 9.75 seconds.\n",
      "Batch 100 took 19.25 seconds.\n",
      "Batch 150 took 29.21 seconds.\n",
      "Batch 200 took 38.81 seconds.\n",
      "Batch 250 took 48.30 seconds.\n",
      "Total runtime: 1106.96, Epoch: 20, took: 56.03 seconds, loss_g: -2452.46, loss_d: -741.78\n"
     ]
    }
   ],
   "source": [
    "path = r'C:\\School\\DELFT\\Graph_ML_project\\data\\swiss-dwellings-v3.0.0'\n",
    "dataset = Transformed_PolyGraphDataset(path)\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "n_batches = len(dataloader)\n",
    "print(n_batches)\n",
    "\n",
    "losses_g, losses_d = train(generator, discriminator, optimizer_G, optimizer_D, dataloader, max_epochs=20)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generator output code\n",
    "\n",
    "The following section contains code to generate the coordinates for a floorplan using the generator.\n",
    "\n",
    "requires:\n",
    "- graph \n",
    "- categories per node (room)\n",
    "- noise vector (of size: NOISE_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator(\n",
      "  (module_list): ModuleList(\n",
      "    (0): TAGConv(141, 64, K=3)\n",
      "    (1): TAGConv(64, 32, K=3)\n",
      "    (2): TAGConv(32, 60, K=3)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Load model\n",
    "model_g_path = r'C:\\School\\DELFT\\Graph_ML_project\\saved_models\\generator.pt'\n",
    "\n",
    "# Make sure the model you're loading in is consistent with the size of your model\n",
    "generator = Generator(input_dim=NOISE_SIZE + len(CATEGORY_DICT), \n",
    "                      output_dim=OUTPUT_GENERATOR, \n",
    "                      hidden_dims=HIDDEN_GENERATOR)\n",
    "\n",
    "generator.load_state_dict(th.load(model_g_path))\n",
    "print(generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(edge_index=[2, 72], geometry=[42, 60], category=[42, 13], num_nodes=42)\n",
      "tensor([4.6750, 3.1847, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 2.2827, 2.8058, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000])\n",
      "torch.Size([42, 60])\n",
      "Data(edge_index=[2, 72], geometry=[42, 60], category=[42, 13], num_nodes=42)\n"
     ]
    }
   ],
   "source": [
    "# Generate floorplan\n",
    "path = r'C:\\School\\DELFT\\Graph_ML_project\\data\\swiss-dwellings-v3.0.0'\n",
    "dataset = Transformed_PolyGraphDataset(path)\n",
    "\n",
    "data = dataset[0]\n",
    "print(data)\n",
    "print(data.geometry[0])\n",
    "\n",
    "noise_vector = th.randn((data.num_nodes, NOISE_SIZE))\n",
    "output = generator(data, noise_vector)\n",
    "print(output.shape)\n",
    "\n",
    "output_data = Data(edge_index=data.edge_index, geometry=output, category=data.category, num_nodes=data.num_nodes)\n",
    "print(output_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualuze floorplan\n",
    "\n",
    "Work in progres..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib.cm import get_cmap\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "\n",
    "FS = 10\n",
    "SUBTYPES_NAMES = [\n",
    "     'Remaining',\n",
    "     'Bathroom',\n",
    "     'Kitchen-Dining',\n",
    "     'Bedroom',\n",
    "     'Corridor',\n",
    "     'Stairs-Ramp',\n",
    "     'Outdoor-Area',\n",
    "     'Living-Room',\n",
    "     'Basement',\n",
    "     'Office',\n",
    "     'Garage',\n",
    "     'Warehouse-Logistics',\n",
    "     'Meeting-Salesroom'\n",
    "]\n",
    "\n",
    "def plot_polygon(ax, poly, **kwargs):\n",
    "    x, y = poly[:MAX_POLYGONS], poly[MAX_POLYGONS:]\n",
    "    ax.fill(x, y, **kwargs)\n",
    "    return\n",
    "\n",
    "def plot_floorplan(ax, areas, area_types, doors=False, walls=False, classes=SUBTYPES_NAMES, colorset='tab20', **kwargs):\n",
    "\n",
    "    cmap = get_cmap(colorset)\n",
    "\n",
    "    for area, area_type in zip(areas, area_types):\n",
    "        try:\n",
    "            np.isnan(area_type)\n",
    "            color_index = len(classes) + 1\n",
    "        except:\n",
    "            color_index = classes.index(area_type)\n",
    "        c=np.array(cmap(color_index)).reshape(1,4)\n",
    "        plot_polygon(ax, area, fc=c, ec=c, label=c, **kwargs)\n",
    "\n",
    "    if walls:\n",
    "        for wall in walls:\n",
    "            plot_polygon(ax, wall, fc='#72246c', ec='#72246c', **kwargs)\n",
    "    if doors:\n",
    "        for door in doors[0]:\n",
    "            plot_polygon(ax, door, fc='red', ec='red', **kwargs)\n",
    "\n",
    "        for door in doors[1]:\n",
    "            plot_polygon(ax, door, fc='orange', ec='orange', **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_polygons(geometries, const=0.5):\n",
    "    polygons = []\n",
    "    for g in geometries:\n",
    "        g[th.abs(g) < const] = 0\n",
    "        xs = g[:MAX_POLYGONS]\n",
    "        ys = g[MAX_POLYGONS:]\n",
    "\n",
    "        current_poly = []\n",
    "        for x, y in zip(xs, ys):\n",
    "            if x != 0 or y != 0:\n",
    "                current_poly.append((x.item(), y.item()))\n",
    "        \n",
    "        polygons.append(current_poly)\n",
    "\n",
    "    return polygons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3, 0, 1, 3, 1, 2, 3, 7, 4, 4, 3, 3, 4, 2, 0, 1, 3, 1, 0, 0, 2, 7, 4, 3,\n",
      "        3, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5])\n",
      "[[(-3.71614670753479, -3.7492172718048096), (0.0, 0.5514585375785828)], [(-15.176667213439941, -15.140957832336426), (5.632940769195557, 5.0686821937561035), (0.0, 0.7534084916114807), (0.70332270860672, 0.5360590219497681), (0.0, -0.5997366905212402), (-0.5675117373466492, 0.0), (1.358695149421692, 0.0), (0.0, 0.5821052193641663), (0.7464379668235779, 0.0), (-0.5926871299743652, 0.7796998023986816), (0.0, 1.8168654441833496), (0.0, -0.9096238613128662), (-0.7175177931785583, 0.0), (0.8363728523254395, 0.849921464920044), (0.0, 0.543422281742096), (0.0, -0.5956943035125732), (0.0, -0.8573947548866272), (0.0, -0.6276554465293884), (0.0, 1.6994669437408447), (0.0, -0.9461386203765869), (-0.8987258672714233, 0.0), (0.0, 1.1647921800613403)], [(-11.09818172454834, -11.22506046295166), (2.784661054611206, 2.0929365158081055), (0.0, -0.5533015131950378), (0.6148145198822021, 0.0), (0.0, 0.8335014581680298), (0.706413209438324, 0.0), (0.519757091999054, -0.7616971731185913), (0.0, 0.5559784173965454), (0.0, 1.1304823160171509), (0.975743293762207, 0.0), (0.0, -0.5068774819374084), (0.0, 1.3694674968719482), (0.5966585278511047, 0.0)], [(-18.717294692993164, -18.583370208740234), (0.0, 8.724797248840332), (0.0, 0.6547812819480896), (0.0, -0.5779366493225098), (-0.5112330317497253, 0.0)], [(-5.876723289489746, -5.862718105316162), (1.5358456373214722, 1.5489157438278198), (0.0, 0.59521484375)], [(-12.32483959197998, -12.351791381835938), (-1.6556987762451172, -1.5578945875167847), (0.0, 0.5038475394248962), (0.0, -0.5054166913032532)], [(-2.6603825092315674, -2.7642128467559814), (2.8352088928222656, 2.6049885749816895), (0.0, 0.5239931344985962), (0.5497681498527527, 0.0), (0.0, 0.56676185131073)], [(0.0, -14.611024856567383), (12.195508003234863, 11.230012893676758), (1.5865042209625244, 1.32382333278656), (-0.7725236415863037, 1.1649988889694214), (0.0, 1.391567349433899), (0.8589043021202087, 0.0), (0.0, 0.811632513999939), (-0.5161989331245422, 0.0), (1.384341835975647, 0.0), (0.5241197347640991, 0.0), (0.0, 0.8301332592964172), (-0.6419275999069214, 0.0), (1.470154881477356, 0.0), (0.0, -0.6454296112060547), (0.0, -0.5893126130104065), (0.0, -0.6189897656440735)], [(-17.42793846130371, -17.02048110961914), (5.180735111236572, 8.06299877166748), (-0.6017223000526428, -0.5970667004585266), (0.0, 1.1222490072250366), (2.6010897159576416, -0.5461403727531433), (0.0, -0.7528393864631653), (0.0, 1.6627123355865479), (0.5633214116096497, 0.0), (0.8055189251899719, -0.5470564365386963), (0.0, 0.654623806476593), (0.0, 0.6583637595176697)], [(0.0, -6.769120693206787), (4.7568745613098145, 0.0), (0.6456671953201294, 0.0), (0.0, -0.7339304089546204), (0.0, 0.7972599267959595)], [(3.8559117317199707, 3.9706437587738037), (0.0, 0.68181312084198), (0.6967954635620117, 0.0), (0.0, 0.6108465194702148), (0.0, 0.5988572835922241)], [(6.737433910369873, 6.295042514801025), (1.8445626497268677, 1.6098308563232422), (0.7318578362464905, 0.0)], [(5.149498462677002, 3.538005828857422), (1.961651086807251, 2.407763957977295), (0.0, 0.5324971675872803), (0.0, -0.9304430484771729), (1.2298190593719482, 0.0), (1.2284533977508545, 0.0), (0.0, 1.2528572082519531), (0.0, -0.5619283318519592), (0.0, 0.7985723614692688), (0.5474323034286499, 0.5143721103668213), (0.0, 1.0376865863800049), (0.7585727572441101, 0.0), (-0.5993971824645996, 0.0), (0.7581173181533813, 0.0), (0.0, 1.2432637214660645), (0.6657000184059143, 0.0)], [(2.3632454872131348, 3.0457844734191895), (0.7040778398513794, 0.0), (0.7245646119117737, 0.0), (0.7121314406394958, 0.0), (0.6168489456176758, 0.0), (0.0, 0.6205819845199585), (0.0, 0.522752046585083), (0.777745246887207, 0.0)], [(0.0, 12.856060028076172), (7.42132043838501, 7.3678178787231445), (0.5773516893386841, 0.0), (0.0, 0.9238024950027466), (0.0, 0.5720359086990356)], [(6.502490997314453, 6.244321823120117), (-5.026220321655273, -5.031596660614014), (1.0729705095291138, -0.5049187541007996), (0.0, 0.5669873356819153), (0.0, 0.9719353318214417), (0.0, 0.5286555290222168), (0.0, 1.0549334287643433), (0.5120304226875305, 0.0)], [(1.250401258468628, 1.9334702491760254), (6.675328254699707, 5.840595245361328), (0.0, 0.6115082502365112), (0.0, 1.2741895914077759)], [(2.616753101348877, 2.1222946643829346), (3.515632152557373, 3.838088274002075), (0.586866021156311, 0.0), (0.0, 0.5258675217628479)], [(-6.172640800476074, -5.874852180480957), (0.0, 3.9095089435577393), (0.0, 0.5933020114898682), (0.7296825051307678, 0.0)], [(-4.349850654602051, -3.6821341514587402), (18.49675750732422, 18.67146110534668), (0.6327049136161804, 0.0), (0.0, 0.6343754529953003), (0.5645551085472107, 0.0), (0.0, 0.5289163589477539), (0.0, 0.6361219882965088), (0.7617840766906738, 0.0), (0.0, 0.5910053849220276)], [(0.0, -1.6171607971191406), (0.0, 0.5334514379501343), (0.0, 0.5471531748771667), (0.0, 0.8617542386054993)], [(5.481391429901123, 5.759979724884033), (0.0, -0.5784056186676025), (0.0, 0.9169749021530151)], [(9.691865921020508, 9.756669044494629), (0.0, 7.915186405181885), (0.0, 1.3085731267929077), (0.5621474981307983, 0.0), (0.0, 0.6198565363883972), (0.0, 0.6293288469314575)], [(6.720073699951172, 6.814144134521484), (0.0, 0.5705824494361877)], [(0.0, 0.528561532497406)], [(9.337751388549805, 8.54971694946289), (-3.1668050289154053, -3.234659194946289), (1.044240951538086, 0.0), (1.2005895376205444, 0.0), (0.0, 0.5596980452537537), (0.0, -0.5104681849479675), (-0.5585277080535889, 0.0), (0.0, 0.739315390586853)], [(6.576324462890625, 0.0), (0.676503598690033, 0.0), (-0.5277711749076843, 1.1161301136016846), (1.0574756860733032, 0.0), (-0.5626848340034485, 0.0), (0.0, 1.0468114614486694), (-0.5627855658531189, 0.0), (-0.6728894710540771, 0.0)], [(1.104169249534607, 0.7432159185409546)], [(-10.694624900817871, -10.650042533874512), (0.6238201260566711, 0.0), (0.0, -0.5265928506851196), (0.5219517946243286, 0.0), (0.0, 0.7552226781845093), (-0.5001095533370972, 0.0)], [(5.947196960449219, 6.163691520690918), (-2.294412612915039, -2.5769858360290527), (0.5799050331115723, 0.0), (0.6445345878601074, 0.0), (0.5293465852737427, 0.0)], [(-4.719169616699219, -4.701960563659668), (0.0, 0.8197684288024902), (0.8127163052558899, 0.0), (0.0, 0.6495603919029236)], [(-10.770466804504395, -10.595948219299316), (10.838334083557129, 10.916751861572266)], [(-8.60238265991211, -8.243391990661621), (-0.6053155064582825, -0.6687633395195007), (0.7112692594528198, 0.0), (0.9947740435600281, 0.0)], [(-3.5428051948547363, -3.6611812114715576), (4.825806617736816, 5.774041175842285), (0.0, 0.6467703580856323), (0.5275862216949463, 0.0), (0.0, 0.674850583076477)], [(-3.9931178092956543, -4.13132905960083), (0.0, -5.678712368011475), (0.0, 0.7675798535346985)], [(7.509878158569336, 7.422922134399414), (-0.8720292448997498, -1.070351481437683), (0.0, 0.6532816886901855), (0.7457948923110962, 0.0), (-0.5275248289108276, 0.0)], [(0.0, -4.6844024658203125), (-8.08520221710205, -8.05322265625), (0.8302112817764282, -0.5669234991073608), (0.0, -0.6333381533622742), (-0.6729540228843689, -0.5099630355834961), (0.0, 1.297956943511963), (-0.5826467275619507, 0.0), (0.0, 0.8539925813674927), (0.0, 2.2759265899658203), (-1.2860652208328247, -1.1756510734558105), (-0.5793420076370239, 0.0), (0.0, 3.072796583175659), (1.7691304683685303, 0.0), (-0.5811604261398315, 0.9987002015113831), (0.0, 1.1560978889465332), (-0.6646727919578552, 0.0), (1.179495096206665, 0.0), (0.0, -0.6027098894119263), (1.0814616680145264, -0.9955320954322815), (0.9613611102104187, 0.0), (0.0, 1.537679672241211)], [(-1.855799674987793, -2.076343536376953), (0.0, -0.8358169198036194)], [(-5.1161675453186035, -5.152266502380371), (-3.5722057819366455, -3.521822929382324)], [(-1.4987767934799194, -1.4324299097061157), (-0.9374091625213623, -0.9679262042045593), (0.0, 0.5079041123390198)], [(-2.727030038833618, -2.7609498500823975), (-1.3031574487686157, -1.1390128135681152), (0.5151970982551575, 0.0)], [(-0.5649848580360413, -0.6375328898429871), (1.1589243412017822, 0.0), (0.0, 0.6281951665878296), (0.0, 1.2891404628753662), (0.0, 0.7069272994995117), (0.7650082111358643, 0.0)]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize floorplan\n",
    "# use output_data = Data(edge_index, geometry, category, num_nodes)\n",
    "categories = output_data.category.argmax(dim=-1)\n",
    "print(categories)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "filtered_polygons = filter_polygons(output_data.geometry.detach())\n",
    "print(filtered_polygons)\n",
    "# filtered_polygons = output_data.geometry.detach()\n",
    "#plot_floorplan(ax, filtered_polygons, categories.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ -3.7161,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "           0.0000,   0.0000,  -3.7492,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "           0.5515,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "           0.0000,   0.0000,   0.0000,   0.0000],\n",
      "        [-15.1767,   5.6329,   0.0000,   0.0000,   0.7033,   0.0000,   0.0000,\n",
      "          -0.5675,   1.3587,   0.0000,   0.0000,   0.0000,   0.0000,   0.7464,\n",
      "          -0.5927,   0.0000,   0.0000,  -0.7175,   0.8364,   0.0000,   0.0000,\n",
      "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,  -0.8987,   0.0000,\n",
      "           0.0000,   0.0000, -15.1410,   5.0687,   0.0000,   0.7534,   0.5361,\n",
      "          -0.5997,   0.0000,   0.0000,   0.0000,   0.5821,   0.0000,   0.0000,\n",
      "           0.0000,   0.0000,   0.7797,   1.8169,  -0.9096,   0.0000,   0.8499,\n",
      "           0.5434,  -0.5957,  -0.8574,  -0.6277,   1.6995,  -0.9461,   0.0000,\n",
      "           0.0000,   0.0000,   0.0000,   1.1648],\n",
      "        [-11.0982,   2.7847,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "           0.0000,   0.0000,   0.6148,   0.0000,   0.0000,   0.7064,   0.5198,\n",
      "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "           0.9757,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "           0.5967,   0.0000, -11.2251,   2.0929,   0.0000,   0.0000,  -0.5533,\n",
      "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.8335,   0.0000,\n",
      "           0.0000,  -0.7617,   0.0000,   0.0000,   0.0000,   0.0000,   0.5560,\n",
      "           0.0000,   1.1305,   0.0000,   0.0000,   0.0000,  -0.5069,   1.3695,\n",
      "           0.0000,   0.0000,   0.0000,   0.0000],\n",
      "        [-18.7173,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,  -0.5112,   0.0000,\n",
      "           0.0000,   0.0000, -18.5834,   8.7248,   0.0000,   0.0000,   0.0000,\n",
      "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "           0.0000,   0.6548,   0.0000,   0.0000,   0.0000,  -0.5779,   0.0000,\n",
      "           0.0000,   0.0000,   0.0000,   0.0000],\n",
      "        [ -5.8767,   1.5358,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "           0.0000,   0.0000,  -5.8627,   1.5489,   0.0000,   0.0000,   0.0000,\n",
      "           0.5952,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "           0.0000,   0.0000,   0.0000,   0.0000]], grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(output_data.geometry[:5])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graph_ml_course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
