{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as th \n",
    "from torch_geometric.data import Data, Batch\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "from model import Generator, Discriminator, gradient_penalty\n",
    "from utils import PolyGraphDataset, Transformed_PolyGraphDataset, CATEGORY_DICT\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Hyperparameters\n",
    "\"\"\"\n",
    "\n",
    "MAX_POLYGONS = 30\n",
    "\n",
    "# Optimizer params\n",
    "g_lr = 0.001 \n",
    "d_lr = 0.001\n",
    "b1 = 0.5 \n",
    "b2 = 0.999  \n",
    "\n",
    "# WGAN params\n",
    "N_critic = 5            # nr of times to train discriminator more\n",
    "lambda_gp = 10          # gradient penalty hyperpraram\n",
    "\n",
    "# Training params\n",
    "MAX_EPOCHS = 500\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "# Network parameters\n",
    "NOISE_SIZE = 128\n",
    "HIDDEN_GENERATOR = [128, 128, 128]\n",
    "OUTPUT_GENERATOR = MAX_POLYGONS * 2             # we want to output at most this many polygons per node (note [x1...y1...] format)\n",
    "\n",
    "HIDDEN_DISCRIMINATOR = [128, 128, 128]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModuleList(\n",
      "  (0): TAGConv(141, 128, K=3)\n",
      "  (1): TAGConv(128, 128, K=3)\n",
      "  (2): TAGConv(128, 60, K=3)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\"\"\" \n",
    "Model Definitions\n",
    "\"\"\"\n",
    "generator = Generator(input_dim=NOISE_SIZE + len(CATEGORY_DICT), \n",
    "                      output_dim=OUTPUT_GENERATOR, \n",
    "                      hidden_dims=HIDDEN_GENERATOR)\n",
    "\n",
    "discriminator = Discriminator(input_dim=OUTPUT_GENERATOR, \n",
    "                              hidden_dims=HIDDEN_DISCRIMINATOR)\n",
    "\n",
    "print(generator.module_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizers\n",
    "optimizer_G = th.optim.Adam(generator.parameters(), lr=g_lr, betas=(b1, b2)) \n",
    "optimizer_D = th.optim.Adam(discriminator.parameters(), lr=d_lr, betas=(b1, b2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main training loop\n",
    "def train(generator, discriminator, optimizer_g, optimizer_d, data_loader):\n",
    "    for epoch in range(MAX_EPOCHS):\n",
    "        # real == batch (confusing naming I know...)\n",
    "        for real in data_loader:\n",
    "            # real = batch of Data() ex. [Data(), Data(), ...] is 1 batch\n",
    "            # Create new data object with noise and same edge_index \n",
    "            print(\"real: \", real)   # should be a Batch() item\n",
    "\n",
    "            for i in range(N_critic):\n",
    "                # Input noise_data into generator\n",
    "                global fake \n",
    "                \n",
    "                noise = th.randn((len(real.category), NOISE_SIZE))\n",
    "                # print(\"noise: \", noise.shape)\n",
    "                fake = generator(real, noise)     \n",
    "\n",
    "                # fake.shape = (batch_size * nodes, output_features = 60)\n",
    "                # We must turn this into appropriate (batch) input for the discriminator\n",
    "                fake = Batch(geometry=fake, edge_index=real.edge_index, batch=real.batch)\n",
    "                \n",
    "                # print(\"fake: \", fake.geometry.shape)\n",
    "                # print(\"real: \", real.geometry.shape)\n",
    "\n",
    "                discriminator_fake = discriminator(fake)    # discriminator scores for fakes\n",
    "                discriminator_real = discriminator(real)    # discriminator scores for reals\n",
    "                \n",
    "                gp = gradient_penalty(discriminator, real, fake)\n",
    "\n",
    "                # Discriminator loss and train\n",
    "                loss_discriminator = -(th.mean(discriminator_real) - th.mean(discriminator_fake)) + lambda_gp * gp\n",
    "                discriminator.zero_grad() \n",
    "                loss_discriminator.backward() \n",
    "                optimizer_d.step()\n",
    "\n",
    "                return 0 \n",
    "\n",
    "            # Generator loss and train\n",
    "            output = discriminator(fake).reshape(-1)        # discriminator scores for fake\n",
    "            loss_generator = -th.mean(output)               # loss for genereator = the discriminators' judgement\n",
    "                                                            # higher score = better\n",
    "            generator.zero_grad()\n",
    "            loss_generator.backward()\n",
    "            optimizer_g.step()\n",
    "    \n",
    "        # TODO: Evaluation and logging code??"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing main training loop\n",
    "\n",
    "Doesnt work yet due to dataloader iteration being broken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "587\n"
     ]
    }
   ],
   "source": [
    "path = r'C:\\School\\DELFT\\Graph_ML_project\\data\\swiss-dwellings-v3.0.0'\n",
    "dataset = Transformed_PolyGraphDataset(path)\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "n_batches = len(dataloader)\n",
    "print(n_batches)\n",
    "\n",
    "# train(generator, discriminator, optimizer_G, optimizer_D, dataloader)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing model for batches of input and gradient penalty for batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(edge_index=[2, 72], geometry=[42, 60], category=[42, 13], door_geometry=[72], num_nodes=42)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataBatch(edge_index=[2, 2304], geometry=[1344, 60], category=[1344, 13], door_geometry=[32], num_nodes=1344, batch=[1344], ptr=[33])\n",
      "noise:  torch.Size([1344, 128])\n",
      "fake:  torch.Size([1344, 60])\n",
      "real:  torch.Size([1344, 60])\n",
      "fake:  DataBatch(edge_index=[2, 2304], geometry=[1344, 60], batch=[1344])\n",
      "real:  DataBatch(edge_index=[2, 2304], geometry=[1344, 60], category=[1344, 13], door_geometry=[32], num_nodes=1344, batch=[1344], ptr=[33])\n",
      "discriminator score fake:  torch.Size([32, 1])\n",
      "discriminator score real:  torch.Size([32, 1])\n",
      "tensor(0.5840, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "from model import gradient_penalty\n",
    "\n",
    "print(dataset[0])\n",
    "test_list = [dataset[0] for _ in range(32)]\n",
    "test_batch = Batch.from_data_list(test_list)\n",
    "print(test_batch)\n",
    "\n",
    "real = test_batch\n",
    "\n",
    "noise = th.randn((len(real.category), NOISE_SIZE))\n",
    "print(\"noise: \", noise.shape)\n",
    "fake = generator(real, noise)     \n",
    "\n",
    "# fake.shape = (batch_size * nodes, output_features = 60)\n",
    "# We must turn this into appropriate (batch) input for the discriminator\n",
    "fake = Batch(geometry=fake, edge_index=real.edge_index, batch=real.batch)\n",
    "\n",
    "print(\"fake: \", fake.geometry.shape)\n",
    "print(\"real: \", real.geometry.shape)\n",
    "\n",
    "discriminator_fake = discriminator(fake)    # discriminator scores for fakes\n",
    "discriminator_real = discriminator(real)    # discriminator scores for reals\n",
    "\n",
    "print(\"fake: \", fake)\n",
    "print(\"real: \", real)\n",
    "print(\"discriminator score fake: \", discriminator_fake.shape)\n",
    "print(\"discriminator score real: \", discriminator_real.shape)\n",
    "\n",
    "gp = gradient_penalty(discriminator, real, fake)\n",
    "print(gp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "tens = th.ones(100)\n",
    "print(th.mean(tens))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing dataloader iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\haora\\anaconda3\\envs\\graph_ml_course\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "  0%|          | 0/587 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/587 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'size'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m dataiter \u001b[39m=\u001b[39m \u001b[39miter\u001b[39m(dataloader)\n\u001b[0;32m      8\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m tqdm(\u001b[39mrange\u001b[39m(n_batches)):\n\u001b[1;32m----> 9\u001b[0m     batch \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39;49m(dataiter)\n\u001b[0;32m     10\u001b[0m     \u001b[39mprint\u001b[39m(batch)\n",
      "File \u001b[1;32mc:\\Users\\haora\\anaconda3\\envs\\graph_ml_course\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    631\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    632\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 633\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    634\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    635\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    636\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\haora\\anaconda3\\envs\\graph_ml_course\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    675\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    676\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 677\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    678\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    679\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\haora\\anaconda3\\envs\\graph_ml_course\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 54\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollate_fn(data)\n",
      "File \u001b[1;32mc:\\Users\\haora\\anaconda3\\envs\\graph_ml_course\\lib\\site-packages\\torch_geometric\\loader\\dataloader.py:20\u001b[0m, in \u001b[0;36mCollater.__call__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m     18\u001b[0m elem \u001b[39m=\u001b[39m batch[\u001b[39m0\u001b[39m]\n\u001b[0;32m     19\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, BaseData):\n\u001b[1;32m---> 20\u001b[0m     \u001b[39mreturn\u001b[39;00m Batch\u001b[39m.\u001b[39;49mfrom_data_list(batch, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfollow_batch,\n\u001b[0;32m     21\u001b[0m                                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mexclude_keys)\n\u001b[0;32m     22\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, torch\u001b[39m.\u001b[39mTensor):\n\u001b[0;32m     23\u001b[0m     \u001b[39mreturn\u001b[39;00m default_collate(batch)\n",
      "File \u001b[1;32mc:\\Users\\haora\\anaconda3\\envs\\graph_ml_course\\lib\\site-packages\\torch_geometric\\data\\batch.py:76\u001b[0m, in \u001b[0;36mBatch.from_data_list\u001b[1;34m(cls, data_list, follow_batch, exclude_keys)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[0;32m     65\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfrom_data_list\u001b[39m(\u001b[39mcls\u001b[39m, data_list: List[BaseData],\n\u001b[0;32m     66\u001b[0m                    follow_batch: Optional[List[\u001b[39mstr\u001b[39m]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m     67\u001b[0m                    exclude_keys: Optional[List[\u001b[39mstr\u001b[39m]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m):\n\u001b[0;32m     68\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Constructs a :class:`~torch_geometric.data.Batch` object from a\u001b[39;00m\n\u001b[0;32m     69\u001b[0m \u001b[39m    Python list of :class:`~torch_geometric.data.Data` or\u001b[39;00m\n\u001b[0;32m     70\u001b[0m \u001b[39m    :class:`~torch_geometric.data.HeteroData` objects.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[39m    :obj:`follow_batch`.\u001b[39;00m\n\u001b[0;32m     74\u001b[0m \u001b[39m    Will exclude any keys given in :obj:`exclude_keys`.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 76\u001b[0m     batch, slice_dict, inc_dict \u001b[39m=\u001b[39m collate(\n\u001b[0;32m     77\u001b[0m         \u001b[39mcls\u001b[39;49m,\n\u001b[0;32m     78\u001b[0m         data_list\u001b[39m=\u001b[39;49mdata_list,\n\u001b[0;32m     79\u001b[0m         increment\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m     80\u001b[0m         add_batch\u001b[39m=\u001b[39;49m\u001b[39mnot\u001b[39;49;00m \u001b[39misinstance\u001b[39;49m(data_list[\u001b[39m0\u001b[39;49m], Batch),\n\u001b[0;32m     81\u001b[0m         follow_batch\u001b[39m=\u001b[39;49mfollow_batch,\n\u001b[0;32m     82\u001b[0m         exclude_keys\u001b[39m=\u001b[39;49mexclude_keys,\n\u001b[0;32m     83\u001b[0m     )\n\u001b[0;32m     85\u001b[0m     batch\u001b[39m.\u001b[39m_num_graphs \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(data_list)\n\u001b[0;32m     86\u001b[0m     batch\u001b[39m.\u001b[39m_slice_dict \u001b[39m=\u001b[39m slice_dict\n",
      "File \u001b[1;32mc:\\Users\\haora\\anaconda3\\envs\\graph_ml_course\\lib\\site-packages\\torch_geometric\\data\\collate.py:85\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(cls, data_list, increment, add_batch, follow_batch, exclude_keys)\u001b[0m\n\u001b[0;32m     82\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n\u001b[0;32m     84\u001b[0m \u001b[39m# Collate attributes into a unified representation:\u001b[39;00m\n\u001b[1;32m---> 85\u001b[0m value, slices, incs \u001b[39m=\u001b[39m _collate(attr, values, data_list, stores,\n\u001b[0;32m     86\u001b[0m                                increment)\n\u001b[0;32m     88\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(value, Tensor) \u001b[39mand\u001b[39;00m value\u001b[39m.\u001b[39mis_cuda:\n\u001b[0;32m     89\u001b[0m     device \u001b[39m=\u001b[39m value\u001b[39m.\u001b[39mdevice\n",
      "File \u001b[1;32mc:\\Users\\haora\\anaconda3\\envs\\graph_ml_course\\lib\\site-packages\\torch_geometric\\data\\collate.py:132\u001b[0m, in \u001b[0;36m_collate\u001b[1;34m(key, values, data_list, stores, increment)\u001b[0m\n\u001b[0;32m    130\u001b[0m \u001b[39mif\u001b[39;00m cat_dim \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m elem\u001b[39m.\u001b[39mdim() \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    131\u001b[0m     values \u001b[39m=\u001b[39m [value\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m) \u001b[39mfor\u001b[39;00m value \u001b[39min\u001b[39;00m values]\n\u001b[1;32m--> 132\u001b[0m slices \u001b[39m=\u001b[39m cumsum([value\u001b[39m.\u001b[39msize(cat_dim \u001b[39mor\u001b[39;00m \u001b[39m0\u001b[39m) \u001b[39mfor\u001b[39;00m value \u001b[39min\u001b[39;00m values])\n\u001b[0;32m    133\u001b[0m \u001b[39mif\u001b[39;00m increment:\n\u001b[0;32m    134\u001b[0m     incs \u001b[39m=\u001b[39m get_incs(key, values, data_list, stores)\n",
      "File \u001b[1;32mc:\\Users\\haora\\anaconda3\\envs\\graph_ml_course\\lib\\site-packages\\torch_geometric\\data\\collate.py:132\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    130\u001b[0m \u001b[39mif\u001b[39;00m cat_dim \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m elem\u001b[39m.\u001b[39mdim() \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    131\u001b[0m     values \u001b[39m=\u001b[39m [value\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m) \u001b[39mfor\u001b[39;00m value \u001b[39min\u001b[39;00m values]\n\u001b[1;32m--> 132\u001b[0m slices \u001b[39m=\u001b[39m cumsum([value\u001b[39m.\u001b[39;49msize(cat_dim \u001b[39mor\u001b[39;00m \u001b[39m0\u001b[39m) \u001b[39mfor\u001b[39;00m value \u001b[39min\u001b[39;00m values])\n\u001b[0;32m    133\u001b[0m \u001b[39mif\u001b[39;00m increment:\n\u001b[0;32m    134\u001b[0m     incs \u001b[39m=\u001b[39m get_incs(key, values, data_list, stores)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'size'"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "\"\"\" \n",
    "Code below produces \"AttributeError: 'list' object has no attribute 'size'\"\n",
    "\"\"\"\n",
    "dataiter = iter(dataloader)\n",
    "\n",
    "for _ in tqdm(range(n_batches)):\n",
    "    batch = next(dataiter)\n",
    "    print(batch)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graph_ml_course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
