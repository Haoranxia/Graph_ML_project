{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as th \n",
    "from torch_geometric.data import Data, Batch\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "from model import Generator, Discriminator, gradient_penalty\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Import Data\n",
    "\"\"\"\n",
    "testing_file = \"./dump/graph-data-examples/2.pickle\"\n",
    "\n",
    "\n",
    "\"\"\" \n",
    "Categories as a dictionary with their indices for onehot encoding\n",
    "\"\"\"\n",
    "CATEGORY_DICT = {\n",
    " 'Remaining': 0,\n",
    " 'Bathroom': 1,\n",
    " 'Kitchen-Dining': 2,\n",
    " 'Bedroom': 3,\n",
    " 'Corridor': 4,\n",
    " 'Stairs-Ramp': 5,\n",
    " 'Outdoor-Area': 6,\n",
    " 'Living-Room': 7,\n",
    " 'Basement': 8,\n",
    " 'Office': 9,\n",
    " 'Garage': 10,\n",
    " 'Warehouse-Logistics': 11,\n",
    " 'Meeting-Salesroom': 12\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(edge_index=[2, 72], geometry=[42], category=[42], centroid=[42, 2], connectivity=[72], door-geometry=[72], walls=[140], num_nodes=42)\n",
      "['Bedroom', 'Remaining', 'Bathroom', 'Bedroom', 'Bathroom', 'Kitchen-Dining', 'Bedroom', 'Living-Room', 'Corridor', 'Corridor', 'Bedroom', 'Bedroom', 'Corridor', 'Kitchen-Dining', 'Remaining', 'Bathroom', 'Bedroom', 'Bathroom', 'Remaining', 'Remaining', 'Kitchen-Dining', 'Living-Room', 'Corridor', 'Bedroom', 'Bedroom', 'Remaining', 'Corridor', 'Remaining', 'Remaining', 'Remaining', 'Remaining', 'Remaining', 'Remaining', 'Remaining', 'Remaining', 'Remaining', 'Remaining', 'Remaining', 'Remaining', 'Remaining', 'Remaining', 'Stairs-Ramp']\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "loaded_data = th.load(testing_file)\n",
    "print(loaded_data)\n",
    "print(loaded_data.category) \n",
    "print(type(loaded_data.geometry))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Hyperparameters\n",
    "\"\"\"\n",
    "\n",
    "MAX_POLYGONS = 30\n",
    "\n",
    "# Optimizer params\n",
    "g_lr = 0.001 \n",
    "d_lr = 0.001\n",
    "b1 = 0.5 \n",
    "b2 = 0.999  \n",
    "\n",
    "# WGAN params\n",
    "N_critic = 5            # nr of times to train discriminator more\n",
    "lambda_gp = 10          # gradient penalty hyperpraram\n",
    "\n",
    "# Training params\n",
    "MAX_EPOCHS = 500\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "# Network parameters\n",
    "NOISE_SIZE = 128\n",
    "HIDDEN_GENERATOR = [128, 128, 128]\n",
    "OUTPUT_GENERATOR = MAX_POLYGONS * 2             # we want to output at most this many polygons per node (note [x1...y1...] format)\n",
    "\n",
    "HIDDEN_DISCRIMINATOR = [128, 128, 128]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehot(categories):\n",
    "    \"\"\" \n",
    "    category: must match CATEGORIES as string\n",
    "    \"\"\"\n",
    "    onehot_encoding = th.zeros((len(categories), len(CATEGORY_DICT)))\n",
    "    for i, cat in enumerate(categories):\n",
    "        onehot_encoding[i][CATEGORY_DICT[cat]] = 1\n",
    "    return onehot_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_input_data = []\n",
    "discriminator_input_data = []\n",
    "\n",
    "def convert_data(data):\n",
    "    # Construct generator data    \n",
    "    onehot_categories = onehot(data.category)\n",
    "    generator_data = Data(x=onehot_categories, edge_index=data.edge_index)\n",
    "\n",
    "    # Construct discriminator data\n",
    "    padded_geometry = th.zeros((len(data.geometry), MAX_POLYGONS * 2), dtype=th.float)\n",
    "    for i, polygon in enumerate(data.geometry):\n",
    "        polygon = th.FloatTensor(polygon)\n",
    "        padded_geometry[i, :polygon.shape[0]] = polygon[:, 0]\n",
    "        padded_geometry[i, MAX_POLYGONS: MAX_POLYGONS + polygon.shape[0]] = polygon[:, 1]\n",
    "\n",
    "    discriminator_data = Data(x=padded_geometry, edge_index=data.edge_index)\n",
    "\n",
    "    return generator_data, discriminator_data\n",
    "\n",
    "generator_data, discriminator_data = convert_data(loaded_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[42, 13], edge_index=[2, 72])\n",
      "Data(x=[42, 60], edge_index=[2, 72])\n"
     ]
    }
   ],
   "source": [
    "\"\"\" \n",
    "Data looks as follows:\n",
    "Generator_data:\n",
    "x = (N nr. of nodes, onehot of categories)\n",
    "\n",
    "Discriminator data:\n",
    "x = (N nr. of nodes, coordinates)\n",
    "where coordinates looks like: [x1, ..., x30, y1, ..., y30]\n",
    "and with 0 padding if (xi, yi) doesn't exist\n",
    "\"\"\"\n",
    "generator_data, discriminator_data = convert_data(loaded_data)\n",
    "\n",
    "print(generator_data)\n",
    "print(discriminator_data)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModuleList(\n",
      "  (0): TAGConv(141, 128, K=3)\n",
      "  (1): TAGConv(128, 128, K=3)\n",
      "  (2): TAGConv(128, 60, K=3)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\"\"\" \n",
    "Model Definitions\n",
    "\"\"\"\n",
    "generator = Generator(input_dim=NOISE_SIZE + len(CATEGORY_DICT), \n",
    "                      output_dim=OUTPUT_GENERATOR, \n",
    "                      hidden_dims=HIDDEN_GENERATOR)\n",
    "\n",
    "discriminator = Discriminator(input_dim=OUTPUT_GENERATOR, \n",
    "                              hidden_dims=HIDDEN_DISCRIMINATOR)\n",
    "\n",
    "print(generator.module_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizers\n",
    "optimizer_G = th.optim.Adam(generator.parameters(), lr=g_lr, betas=(b1, b2)) \n",
    "optimizer_D = th.optim.Adam(discriminator.parameters(), lr=d_lr, betas=(b1, b2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main training loop\n",
    "def train(generator, discriminator, optimizer_g, optimizer_d, data_loader):\n",
    "    for epoch in range(MAX_EPOCHS):\n",
    "        # real == batch (confusing naming I know...)\n",
    "        for real in data_loader:\n",
    "            # Create new data object with noise and same edge_index \n",
    "            for i in range(N_critic):\n",
    "                noise_batch = []\n",
    "\n",
    "                # Create n_batch number of noise_vectors and batch it\n",
    "                # We must create a noise vector with the corresponding graph in the actual data\n",
    "                for batch_data in real:\n",
    "                    noise = th.randn(NOISE_SIZE)\n",
    "                    noise = Data(x=noise, edge_index=batch_data.edge_indices)\n",
    "                    noise_batch.append(noise)\n",
    "\n",
    "                noise_batch = Batch.from_data_list(noise_batch)\n",
    "\n",
    "                # Input noise_data into generator\n",
    "                global fake \n",
    "                fake = Generator(noise_batch)\n",
    "\n",
    "                # Generator output is a tensor of dimensionality: (sum of all nodes in batch, output_features)\n",
    "                # We must turn this into appropriate (batch) input for the discriminator\n",
    "                splits = [batch_data.shape[0] for batch_data in real]       # nr of nodes per graph\n",
    "                fake_batch = th.split(fake, splits, dim=0)                  # split stacked tensor fake into appropriate batches\n",
    "                fake = Batch.from_data_list(fake_batch)\n",
    "\n",
    "                discriminator_fake = discriminator(fake).reshape(-1)        # discriminator scores for fakes\n",
    "                discriminator_real = discriminator(real).reshape(-1)        # discriminator scores for reals\n",
    "                gp = gradient_penalty(discriminator, real, fake)\n",
    "\n",
    "                # Discriminator loss and train\n",
    "                loss_discriminator = -(th.mean(discriminator_real) - th.mean(discriminator_fake)) + lambda_gp * gp\n",
    "                discriminator.zero_grad() \n",
    "                loss_discriminator.backward() \n",
    "                optimizer_d.step()\n",
    "\n",
    "            # Generator loss and train\n",
    "            output = discriminator(fake).reshape(-1)        # discriminator scores for fake\n",
    "            loss_generator = -th.mean(output)               # loss for genereator = the discriminators' judgement\n",
    "                                                            # higher score = better\n",
    "            generator.zero_grad()\n",
    "            loss_generator.backward()\n",
    "            optimizer_g.step()\n",
    "    \n",
    "        # TODO: Evaluation and logging code??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[42, 13], edge_index=[2, 72])\n",
      "Data(x=[42, 60], edge_index=[2, 72])\n",
      "torch.Size([42, 141])\n",
      "torch.Size([42, 60])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 1])\n"
     ]
    }
   ],
   "source": [
    "print(generator_data)\n",
    "print(discriminator_data)\n",
    "\n",
    "# Generator testing\n",
    "noise_vector = th.randn(generator_data.x.shape[0], NOISE_SIZE)\n",
    "generator_input = th.concat((noise_vector, generator_data.x), dim=1)\n",
    "print(generator_input.shape)\n",
    "\n",
    "generator_input = Data(x=generator_input, edge_index=generator_data.edge_index)\n",
    "generator_out = generator(generator_input)\n",
    "\n",
    "print(generator_out.shape)\n",
    "\n",
    "# Discriminator testing with Generator output\n",
    "discriminator_input = Data(x=generator_out, edge_index=discriminator_data.edge_index)\n",
    "discriminator_out = discriminator(discriminator_input)\n",
    "print(discriminator_out.shape)\n",
    "\n",
    "# Discriminator testing with discriminator input (ground truth)\n",
    "discriminator_out = discriminator(discriminator_data)\n",
    "print(discriminator_out.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graph_ml_course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
