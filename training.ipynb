{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\haora\\anaconda3\\envs\\graph_ml_course\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch as th \n",
    "from torch_geometric.data import Data, Batch\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "from model import Generator, Discriminator, gradient_penalty\n",
    "from utils import Transformed_PolyGraphDataset, CATEGORY_DICT\n",
    "\n",
    "import numpy as np\n",
    "import time "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Hyperparameters\n",
    "\"\"\"\n",
    "\n",
    "MAX_POLYGONS = 30       # max nr. of polygons to output\n",
    "\n",
    "# Optimizer params\n",
    "g_lr = 0.001 \n",
    "d_lr = 0.001\n",
    "b1 = 0.5 \n",
    "b2 = 0.999  \n",
    "\n",
    "# WGAN params\n",
    "N_critic = 5            # nr of times to train discriminator more\n",
    "lambda_gp = 10          # gradient penalty hyperpraram\n",
    "\n",
    "# Training params\n",
    "MAX_EPOCHS = 50\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Network parameters\n",
    "NOISE_SIZE = 128\n",
    "HIDDEN_GENERATOR = [64, 32, 32]                 # list of dimensions for hidden layers\n",
    "OUTPUT_GENERATOR = MAX_POLYGONS * 2             # we want to output at most this many polygons per node (note [x1... y1...] format)\n",
    "\n",
    "HIDDEN_DISCRIMINATOR = [64, 32, 16]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model definition\n",
    "\n",
    "### Generator\n",
    "- Input of Generator will always be: \"noise size + Nr. of categories\" \n",
    "- Output of Generator = (MAX_POYLGONS * 2) due to our output coordinate format\n",
    "\n",
    "\n",
    "### Discriminator\n",
    "- Input of Discriminator = output of Generator (MAX_POLYGONS * 2) due to our output coordinate format\n",
    "- Output of Discriminator will always be 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModuleList(\n",
      "  (0): TAGConv(141, 64, K=3)\n",
      "  (1): TAGConv(64, 32, K=3)\n",
      "  (2): TAGConv(32, 60, K=3)\n",
      ")\n",
      "ModuleList(\n",
      "  (0): TAGConv(60, 64, K=3)\n",
      "  (1): TAGConv(64, 32, K=3)\n",
      "  (2): TAGConv(32, 16, K=3)\n",
      "  (3): Linear(in_features=16, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\"\"\" \n",
    "Model Definitions\n",
    "\"\"\"\n",
    "generator = Generator(input_dim=NOISE_SIZE + len(CATEGORY_DICT), \n",
    "                      output_dim=OUTPUT_GENERATOR, \n",
    "                      hidden_dims=HIDDEN_GENERATOR)\n",
    "\n",
    "discriminator = Discriminator(input_dim=OUTPUT_GENERATOR, \n",
    "                              hidden_dims=HIDDEN_DISCRIMINATOR)\n",
    "\n",
    "print(generator.module_list)\n",
    "print(discriminator.module_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizers\n",
    "optimizer_G = th.optim.Adam(generator.parameters(), lr=g_lr, betas=(b1, b2)) \n",
    "optimizer_D = th.optim.Adam(discriminator.parameters(), lr=d_lr, betas=(b1, b2))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training loop\n",
    "\n",
    "Generator details:\n",
    "- We generate a noise vector\n",
    "- Generator uses noise vector and training data (the categories) as input\n",
    "- Generator uses graph NN layers to generate coordinates out of the noise vectors using the graph structure and categories\n",
    "- Generator output: (MAX_POLYGONS * 2) for each node in the input\n",
    "\n",
    "Discriminator details:\n",
    "- Generate discriminator output (score) for real data and fake (generated) data\n",
    "- Compute loss over these scores with additional gradient penalty loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(generator, discriminator, optimizer_g, optimizer_d, data_loader):\n",
    "\n",
    "    losses_g, losses_d = [], []\n",
    "    num_steps = 0\n",
    "\n",
    "    # real = batch of Data() ex. [Data(), Data(), ...] is 1 batch\n",
    "    start_time = time.time()\n",
    "    for i, real in enumerate(data_loader):\n",
    "        num_steps += 1\n",
    "\n",
    "        # Input noise_data into generator            \n",
    "        noise = th.randn((len(real.category), NOISE_SIZE))\n",
    "        fake = generator(real, noise)     \n",
    "\n",
    "        # fake.shape = (batch_size * nodes, output_features = 60)\n",
    "        # We must turn this into appropriate (batch) input for the discriminator\n",
    "        fake = Batch(geometry=fake, edge_index=real.edge_index, batch=real.batch)\n",
    "\n",
    "        discriminator_fake = discriminator(fake)    # discriminator scores for fakes\n",
    "        discriminator_real = discriminator(real)    # discriminator scores for reals\n",
    "        \n",
    "        gp = gradient_penalty(discriminator, real, fake)\n",
    "\n",
    "        # Discriminator loss and train\n",
    "        loss_discriminator = -(th.mean(discriminator_real) - th.mean(discriminator_fake)) + lambda_gp * gp\n",
    "        \n",
    "        discriminator.zero_grad() \n",
    "        loss_discriminator.backward() \n",
    "        optimizer_d.step()\n",
    "\n",
    "        losses_d.append(loss_discriminator.item())\n",
    "\n",
    "        # Only train Generator every 5 steps\n",
    "        if num_steps % N_critic == 0:\n",
    "            noise_g = th.randn((len(real.category), NOISE_SIZE))\n",
    "            fake_g = generator(real, noise_g) \n",
    "            fake_g = Batch(geometry=fake_g, edge_index=real.edge_index, batch=real.batch)\n",
    "\n",
    "            output = discriminator(fake_g).reshape(-1)      # discriminator scores for fake\n",
    "            loss_generator = -th.mean(output)               # loss for genereator = the discriminators' judgement\n",
    "                                                            # higher score = better\n",
    "            generator.zero_grad()\n",
    "            loss_generator.backward()\n",
    "            optimizer_g.step()\n",
    "\n",
    "            losses_g.append(loss_generator.item())\n",
    "        \n",
    "        # 49 is just a temporary number\n",
    "        if i % 49 == 0:\n",
    "            batch_time = time.time() - start_time\n",
    "            print(\"Batch {} took {:.2f} seconds.\".format(i, batch_time))\n",
    "    \n",
    "    return sum(losses_g) / len(losses_g), sum(losses_d) / len(losses_d)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main training loop\n",
    "\n",
    "Code could be improved. Make sure to specify your own 'generator_path', 'discriminator_path'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main training loop\n",
    "def train(generator, discriminator, optimizer_g, optimizer_d, data_loader, max_epochs):\n",
    "    start_time = time.time() \n",
    "    losses_g, losses_d = [], []\n",
    "\n",
    "    for epoch in range(1, max_epochs + 1):\n",
    "        epoch_time = time.time() \n",
    "\n",
    "        loss_g, loss_d = run_epoch(generator, discriminator, optimizer_g, optimizer_d, data_loader)\n",
    "        losses_g.append(loss_g.item())\n",
    "        losses_d.append(loss_d.item())\n",
    "\n",
    "        # TODO: Evaluation and logging code??\n",
    "        epoch_time = time.time() - epoch_time \n",
    "        total_time = time.time() - start_time\n",
    "        print(\"Total runtime: {:.2f}, Epoch: {}, took: {:.2f} seconds, loss_g: {:.2f}, loss_d: {:.2f}\".format(total_time, epoch, epoch_time, loss_g, loss_d))\n",
    "\n",
    "        # save model every 10 epochs\n",
    "        if epoch % 10:\n",
    "            print(\"Saving model at epoch {}\".format(epoch))\n",
    "            generator_path = r'C:\\School\\DELFT\\Graph_ML_project\\saved_models\\generator.pt'\n",
    "            discriminator_path = r'C:\\School\\DELFT\\Graph_ML_project\\saved_models\\discriminator.pt'\n",
    "            th.save(generator.state_dict(), generator_path)\n",
    "            th.save(discriminator.state_dict(), discriminator_path)\n",
    "\n",
    "    return losses_g, losses_d"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "294\n",
      "Batch 0 took 0.22 seconds.\n",
      "Batch 49 took 9.63 seconds.\n",
      "Batch 98 took 19.45 seconds.\n",
      "Batch 147 took 29.03 seconds.\n",
      "Batch 196 took 38.33 seconds.\n",
      "Batch 245 took 47.74 seconds.\n",
      "Total runtime: 56.96, Epoch: 1, took: 56.96 seconds, loss_g: -369.94, loss_d: -4111.66\n",
      "Batch 0 took 0.21 seconds.\n",
      "Batch 49 took 9.61 seconds.\n",
      "Batch 98 took 19.33 seconds.\n",
      "Batch 147 took 29.29 seconds.\n",
      "Batch 196 took 39.20 seconds.\n",
      "Batch 245 took 49.07 seconds.\n",
      "Total runtime: 115.83, Epoch: 2, took: 58.86 seconds, loss_g: -248.54, loss_d: -3447.27\n",
      "Batch 0 took 0.26 seconds.\n",
      "Batch 49 took 10.38 seconds.\n",
      "Batch 98 took 20.72 seconds.\n",
      "Batch 147 took 31.20 seconds.\n",
      "Batch 196 took 42.02 seconds.\n",
      "Batch 245 took 53.28 seconds.\n",
      "Total runtime: 179.80, Epoch: 3, took: 63.97 seconds, loss_g: -171.92, loss_d: -1748.79\n",
      "Batch 0 took 0.27 seconds.\n",
      "Batch 49 took 11.58 seconds.\n",
      "Batch 98 took 23.00 seconds.\n",
      "Batch 147 took 34.77 seconds.\n",
      "Batch 196 took 46.50 seconds.\n",
      "Batch 245 took 58.80 seconds.\n",
      "Total runtime: 251.89, Epoch: 4, took: 72.08 seconds, loss_g: -144.89, loss_d: -1965.17\n",
      "Batch 0 took 0.28 seconds.\n",
      "Batch 49 took 15.12 seconds.\n",
      "Batch 98 took 28.25 seconds.\n",
      "Batch 147 took 41.89 seconds.\n",
      "Batch 196 took 55.58 seconds.\n",
      "Batch 245 took 69.32 seconds.\n",
      "Total runtime: 334.80, Epoch: 5, took: 82.91 seconds, loss_g: -92.18, loss_d: -1754.56\n",
      "Batch 0 took 0.26 seconds.\n",
      "Batch 49 took 14.53 seconds.\n",
      "Batch 98 took 29.40 seconds.\n",
      "Batch 147 took 44.13 seconds.\n",
      "Batch 196 took 59.35 seconds.\n",
      "Batch 245 took 74.74 seconds.\n",
      "Total runtime: 424.58, Epoch: 6, took: 89.78 seconds, loss_g: 72.79, loss_d: -1486.49\n",
      "Batch 0 took 0.33 seconds.\n",
      "Batch 49 took 16.52 seconds.\n",
      "Batch 98 took 33.04 seconds.\n",
      "Batch 147 took 49.63 seconds.\n",
      "Batch 196 took 66.48 seconds.\n",
      "Batch 245 took 83.76 seconds.\n",
      "Total runtime: 525.44, Epoch: 7, took: 100.85 seconds, loss_g: 298.03, loss_d: -1348.92\n",
      "Batch 0 took 0.32 seconds.\n",
      "Batch 49 took 18.68 seconds.\n",
      "Batch 98 took 37.24 seconds.\n",
      "Batch 147 took 57.05 seconds.\n",
      "Batch 196 took 76.02 seconds.\n",
      "Batch 245 took 95.85 seconds.\n",
      "Total runtime: 641.14, Epoch: 8, took: 115.70 seconds, loss_g: 744.11, loss_d: -1208.33\n",
      "Batch 0 took 0.44 seconds.\n",
      "Batch 49 took 21.69 seconds.\n",
      "Batch 98 took 43.33 seconds.\n",
      "Batch 147 took 65.32 seconds.\n",
      "Batch 196 took 88.28 seconds.\n",
      "Batch 245 took 111.67 seconds.\n",
      "Total runtime: 775.70, Epoch: 9, took: 134.55 seconds, loss_g: 861.54, loss_d: -978.12\n",
      "Batch 0 took 0.50 seconds.\n",
      "Batch 49 took 25.37 seconds.\n",
      "Batch 98 took 51.16 seconds.\n",
      "Batch 147 took 77.14 seconds.\n",
      "Batch 196 took 103.71 seconds.\n",
      "Batch 245 took 132.72 seconds.\n",
      "Total runtime: 934.60, Epoch: 10, took: 158.89 seconds, loss_g: 592.15, loss_d: -759.12\n",
      "Batch 0 took 0.53 seconds.\n",
      "Batch 49 took 28.51 seconds.\n",
      "Batch 98 took 56.17 seconds.\n",
      "Batch 147 took 85.00 seconds.\n",
      "Batch 196 took 114.23 seconds.\n",
      "Batch 245 took 144.20 seconds.\n",
      "Total runtime: 1108.53, Epoch: 11, took: 173.93 seconds, loss_g: 563.76, loss_d: -591.31\n",
      "Batch 0 took 0.66 seconds.\n",
      "Batch 49 took 32.30 seconds.\n",
      "Batch 98 took 66.32 seconds.\n",
      "Batch 147 took 98.70 seconds.\n",
      "Batch 196 took 131.90 seconds.\n",
      "Batch 245 took 165.84 seconds.\n",
      "Total runtime: 1307.79, Epoch: 12, took: 199.25 seconds, loss_g: 210.91, loss_d: -532.77\n",
      "Batch 0 took 0.73 seconds.\n",
      "Batch 49 took 35.73 seconds.\n",
      "Batch 98 took 70.72 seconds.\n",
      "Batch 147 took 106.83 seconds.\n",
      "Batch 196 took 143.50 seconds.\n",
      "Batch 245 took 180.97 seconds.\n",
      "Total runtime: 1525.67, Epoch: 13, took: 217.88 seconds, loss_g: 60.80, loss_d: -475.00\n",
      "Batch 0 took 0.79 seconds.\n",
      "Batch 49 took 39.26 seconds.\n",
      "Batch 98 took 78.47 seconds.\n",
      "Batch 147 took 118.33 seconds.\n",
      "Batch 196 took 158.29 seconds.\n",
      "Batch 245 took 198.49 seconds.\n",
      "Total runtime: 1764.06, Epoch: 14, took: 238.38 seconds, loss_g: -126.49, loss_d: -451.36\n",
      "Batch 0 took 0.86 seconds.\n",
      "Batch 49 took 42.80 seconds.\n",
      "Batch 98 took 84.56 seconds.\n",
      "Batch 147 took 136.00 seconds.\n",
      "Batch 196 took 180.69 seconds.\n",
      "Batch 245 took 225.00 seconds.\n",
      "Total runtime: 2032.42, Epoch: 15, took: 268.35 seconds, loss_g: -345.97, loss_d: -426.44\n",
      "Batch 0 took 0.89 seconds.\n",
      "Batch 49 took 46.42 seconds.\n",
      "Batch 98 took 91.55 seconds.\n",
      "Batch 147 took 137.87 seconds.\n",
      "Batch 196 took 184.53 seconds.\n",
      "Batch 245 took 232.24 seconds.\n",
      "Total runtime: 2314.36, Epoch: 16, took: 281.93 seconds, loss_g: -275.82, loss_d: -420.14\n",
      "Batch 0 took 0.96 seconds.\n",
      "Batch 49 took 53.63 seconds.\n",
      "Batch 98 took 102.59 seconds.\n",
      "Batch 147 took 152.49 seconds.\n",
      "Batch 196 took 203.60 seconds.\n",
      "Batch 245 took 255.31 seconds.\n",
      "Total runtime: 2618.73, Epoch: 17, took: 304.37 seconds, loss_g: -399.52, loss_d: -409.62\n",
      "Batch 0 took 1.06 seconds.\n",
      "Batch 49 took 51.99 seconds.\n",
      "Batch 98 took 103.81 seconds.\n",
      "Batch 147 took 156.51 seconds.\n",
      "Batch 196 took 209.86 seconds.\n",
      "Batch 245 took 264.15 seconds.\n",
      "Total runtime: 2935.15, Epoch: 18, took: 316.40 seconds, loss_g: -510.29, loss_d: -404.14\n",
      "Batch 0 took 1.01 seconds.\n",
      "Batch 49 took 55.12 seconds.\n",
      "Batch 98 took 110.68 seconds.\n",
      "Batch 147 took 165.64 seconds.\n",
      "Batch 196 took 221.33 seconds.\n",
      "Batch 245 took 279.85 seconds.\n",
      "Total runtime: 3270.09, Epoch: 19, took: 334.94 seconds, loss_g: -709.43, loss_d: -423.50\n",
      "Batch 0 took 1.19 seconds.\n",
      "Batch 49 took 58.98 seconds.\n",
      "Batch 98 took 116.87 seconds.\n",
      "Batch 147 took 175.75 seconds.\n",
      "Batch 196 took 235.42 seconds.\n",
      "Batch 245 took 295.78 seconds.\n",
      "Total runtime: 3624.20, Epoch: 20, took: 354.10 seconds, loss_g: -1092.76, loss_d: -462.23\n"
     ]
    }
   ],
   "source": [
    "path = r'C:\\School\\DELFT\\Graph_ML_project\\data\\swiss-dwellings-v3.0.0'\n",
    "dataset = Transformed_PolyGraphDataset(path)\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "n_batches = len(dataloader)\n",
    "print(n_batches)\n",
    "\n",
    "losses_g, losses_d = train(generator, discriminator, optimizer_G, optimizer_D, dataloader, max_epochs=20)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generator output code\n",
    "\n",
    "The following section contains code to generate the coordinates for a floorplan using the generator.\n",
    "\n",
    "requires:\n",
    "- graph \n",
    "- categories per node (room)\n",
    "- noise vector (of size: NOISE_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator(\n",
      "  (module_list): ModuleList(\n",
      "    (0): TAGConv(141, 64, K=3)\n",
      "    (1): TAGConv(64, 32, K=3)\n",
      "    (2): TAGConv(32, 60, K=3)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Load model\n",
    "model_g_path = r'C:\\School\\DELFT\\Graph_ML_project\\saved_models\\generator.pt'\n",
    "\n",
    "# Make sure the model you're loading in is consistent with the size of your model\n",
    "generator = Generator(input_dim=NOISE_SIZE + len(CATEGORY_DICT), \n",
    "                      output_dim=OUTPUT_GENERATOR, \n",
    "                      hidden_dims=HIDDEN_GENERATOR)\n",
    "\n",
    "generator.load_state_dict(th.load(model_g_path))\n",
    "print(generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(edge_index=[2, 72], geometry=[42, 60], category=[42, 13], num_nodes=42)\n",
      "torch.Size([42, 60])\n",
      "Data(edge_index=[2, 72], geometry=[42, 60], category=[42, 13], num_nodes=42)\n"
     ]
    }
   ],
   "source": [
    "# Generate floorplan\n",
    "path = r'C:\\School\\DELFT\\Graph_ML_project\\data\\swiss-dwellings-v3.0.0'\n",
    "dataset = Transformed_PolyGraphDataset(path)\n",
    "\n",
    "data = dataset[0]\n",
    "print(data)\n",
    "\n",
    "noise_vector = th.randn((data.num_nodes, NOISE_SIZE))\n",
    "output = generator(data, noise_vector)\n",
    "print(output.shape)\n",
    "\n",
    "output_data = Data(edge_index=data.edge_index, geometry=output, category=data.category, num_nodes=data.num_nodes)\n",
    "print(output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3, 0, 1, 3, 1, 2, 3, 7, 4, 4, 3, 3, 4, 2, 0, 1, 3, 1, 0, 0, 2, 7, 4, 3,\n",
      "        3, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5])\n",
      "tensor([[-5.5891e+00,  2.9485e+00, -3.9081e-02,  ..., -1.7712e-01,\n",
      "         -1.0998e-01, -2.2612e-02],\n",
      "        [-3.7683e+00,  1.6112e+01, -2.5324e-03,  ..., -2.4961e-01,\n",
      "         -4.7313e-02,  2.7180e-01],\n",
      "        [ 8.0758e-01,  0.0000e+00, -0.0000e+00,  ..., -2.0717e-01,\n",
      "         -1.2610e-01, -7.1626e-02],\n",
      "        ...,\n",
      "        [-7.5371e+00, -7.3328e+00, -7.2917e-02,  ..., -2.3652e-01,\n",
      "         -0.0000e+00, -1.9689e-01],\n",
      "        [-5.4364e+00,  3.5668e+00,  1.9171e-01,  ..., -0.0000e+00,\n",
      "         -1.3893e-01, -1.0629e-01],\n",
      "        [-0.0000e+00, -6.2440e+00, -1.0697e-01,  ..., -5.2443e-01,\n",
      "         -1.3695e-01, -2.5121e-02]], grad_fn=<PreluKernelBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Visualize floorplan\n",
    "# use output_data = Data(edge_index, geometry, category, num_nodes)\n",
    "from plot import plot_floorplan\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "categories = output_data.category.argmax(dim=-1)\n",
    "print(categories)\n",
    "print(output_data.geometry)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graph_ml_course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
