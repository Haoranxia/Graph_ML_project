{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as th \n",
    "from torch_geometric.data import Data, Batch\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "from model import Generator, Discriminator, gradient_penalty\n",
    "from utils import PolyGraphDataset, Transformed_PolyGraphDataset, CATEGORY_DICT\n",
    "\n",
    "import numpy as np\n",
    "import time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Hyperparameters\n",
    "\"\"\"\n",
    "\n",
    "MAX_POLYGONS = 30\n",
    "\n",
    "# Optimizer params\n",
    "g_lr = 0.001 \n",
    "d_lr = 0.001\n",
    "b1 = 0.5 \n",
    "b2 = 0.999  \n",
    "\n",
    "# WGAN params\n",
    "N_critic = 5            # nr of times to train discriminator more\n",
    "lambda_gp = 10          # gradient penalty hyperpraram\n",
    "\n",
    "# Training params\n",
    "MAX_EPOCHS = 50\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Network parameters\n",
    "NOISE_SIZE = 128\n",
    "HIDDEN_GENERATOR = [32, 64, 32]\n",
    "OUTPUT_GENERATOR = MAX_POLYGONS * 2             # we want to output at most this many polygons per node (note [x1...y1...] format)\n",
    "\n",
    "HIDDEN_DISCRIMINATOR = [32, 64, 32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModuleList(\n",
      "  (0): TAGConv(141, 128, K=3)\n",
      "  (1): TAGConv(128, 128, K=3)\n",
      "  (2): TAGConv(128, 60, K=3)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\"\"\" \n",
    "Model Definitions\n",
    "\"\"\"\n",
    "generator = Generator(input_dim=NOISE_SIZE + len(CATEGORY_DICT), \n",
    "                      output_dim=OUTPUT_GENERATOR, \n",
    "                      hidden_dims=HIDDEN_GENERATOR)\n",
    "\n",
    "discriminator = Discriminator(input_dim=OUTPUT_GENERATOR, \n",
    "                              hidden_dims=HIDDEN_DISCRIMINATOR)\n",
    "\n",
    "print(generator.module_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizers\n",
    "optimizer_G = th.optim.Adam(generator.parameters(), lr=g_lr, betas=(b1, b2)) \n",
    "optimizer_D = th.optim.Adam(discriminator.parameters(), lr=d_lr, betas=(b1, b2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(generator, discriminator, optimizer_g, optimizer_d, data_loader):\n",
    "    losses_g, losses_d = [], []\n",
    "\n",
    "    for real in data_loader:\n",
    "    # real = batch of Data() ex. [Data(), Data(), ...] is 1 batch\n",
    "    \n",
    "        for _ in range(N_critic):\n",
    "            # Input noise_data into generator\n",
    "            global fake \n",
    "            \n",
    "            noise = th.randn((len(real.category), NOISE_SIZE))\n",
    "            fake = generator(real, noise)     \n",
    "\n",
    "            # fake.shape = (batch_size * nodes, output_features = 60)\n",
    "            # We must turn this into appropriate (batch) input for the discriminator\n",
    "            fake = Batch(geometry=fake, edge_index=real.edge_index, batch=real.batch)\n",
    "\n",
    "            discriminator_fake = discriminator(fake)    # discriminator scores for fakes\n",
    "            discriminator_real = discriminator(real)    # discriminator scores for reals\n",
    "            \n",
    "            gp = gradient_penalty(discriminator, real, fake)\n",
    "\n",
    "            # Discriminator loss and train\n",
    "            loss_discriminator = -(th.mean(discriminator_real) - th.mean(discriminator_fake)) + lambda_gp * gp\n",
    "            \n",
    "            discriminator.zero_grad() \n",
    "            loss_discriminator.backward(retain_graph=True) \n",
    "            optimizer_d.step()\n",
    "\n",
    "            # Save discriminator loss\n",
    "            losses_d.append(loss_discriminator)\n",
    "\n",
    "        # Generator loss and train\n",
    "        output = discriminator(fake).reshape(-1)        # discriminator scores for fake\n",
    "        loss_generator = -th.mean(output)               # loss for genereator = the discriminators' judgement\n",
    "                                                        # higher score = better\n",
    "        generator.zero_grad()\n",
    "        loss_generator.backward()\n",
    "        optimizer_g.step()\n",
    "\n",
    "        # Save generator loss\n",
    "        losses_g.append(loss_generator)\n",
    "\n",
    "        # Release memory of retained graph\n",
    "        loss_discriminator.detach()\n",
    "    \n",
    "    return sum(losses_g) / len(losses_g), sum(losses_d) / len(losses_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main training loop\n",
    "def train(generator, discriminator, optimizer_g, optimizer_d, data_loader, max_epochs):\n",
    "    start_time = time.time() \n",
    "    losses_g, losses_d = [], []\n",
    "\n",
    "    for epoch in range(1, max_epochs + 1):\n",
    "        epoch_time = time.time() \n",
    "\n",
    "        loss_g, loss_d = run_epoch(generator, discriminator, optimizer_g, optimizer_d, data_loader)\n",
    "        losses_g.append(loss_g)\n",
    "        losses_d.append(loss_d)\n",
    "\n",
    "        # TODO: Evaluation and logging code??\n",
    "        epoch_time = time.time() - epoch_time \n",
    "        total_time = time.time() - start_time\n",
    "        print(\"Total runtime: {:.2f}, Epoch: {}, took: {:.2f} seconds, loss_g: {:.2f}, loss_d: {:.2f}\".format(total_time, epoch, epoch_time, loss_g, loss_d))\n",
    "\n",
    "        # save model every 10 epochs\n",
    "        if epoch % 10:\n",
    "            generator_path = r'C:\\School\\DELFT\\Graph_ML_project\\saved_models\\generator.pt'\n",
    "            discriminator_path = r'C:\\School\\DELFT\\Graph_ML_project\\saved_models\\discriminator.pt'\n",
    "            th.save(generator.state_dict(), generator_path)\n",
    "            th.save(discriminator.state_dict(), discriminator_path)\n",
    "\n",
    "    return losses_g, losses_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "294\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m n_batches \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(dataloader)\n\u001b[0;32m      6\u001b[0m \u001b[39mprint\u001b[39m(n_batches)\n\u001b[1;32m----> 8\u001b[0m losses_g, losses_d \u001b[39m=\u001b[39m train(generator, discriminator, optimizer_G, optimizer_D, dataloader, max_epochs\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m)\n",
      "Cell \u001b[1;32mIn[6], line 9\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(generator, discriminator, optimizer_g, optimizer_d, data_loader, max_epochs)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, max_epochs \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m):\n\u001b[0;32m      7\u001b[0m     epoch_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \n\u001b[1;32m----> 9\u001b[0m     loss_g, loss_d \u001b[39m=\u001b[39m run_epoch(generator, discriminator, optimizer_g, optimizer_d, data_loader)\n\u001b[0;32m     10\u001b[0m     losses_g\u001b[39m.\u001b[39mappend(loss_g)\n\u001b[0;32m     11\u001b[0m     losses_d\u001b[39m.\u001b[39mappend(loss_d)\n",
      "Cell \u001b[1;32mIn[5], line 22\u001b[0m, in \u001b[0;36mrun_epoch\u001b[1;34m(generator, discriminator, optimizer_g, optimizer_d, data_loader)\u001b[0m\n\u001b[0;32m     17\u001b[0m fake \u001b[39m=\u001b[39m Batch(geometry\u001b[39m=\u001b[39mfake, edge_index\u001b[39m=\u001b[39mreal\u001b[39m.\u001b[39medge_index, batch\u001b[39m=\u001b[39mreal\u001b[39m.\u001b[39mbatch)\n\u001b[0;32m     19\u001b[0m \u001b[39m# print(\"fake: \", fake.geometry.shape)\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[39m# print(\"real: \", real.geometry.shape)\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m discriminator_fake \u001b[39m=\u001b[39m discriminator(fake)    \u001b[39m# discriminator scores for fakes\u001b[39;00m\n\u001b[0;32m     23\u001b[0m discriminator_real \u001b[39m=\u001b[39m discriminator(real)    \u001b[39m# discriminator scores for reals\u001b[39;00m\n\u001b[0;32m     25\u001b[0m gp \u001b[39m=\u001b[39m gradient_penalty(discriminator, real, fake)\n",
      "File \u001b[1;32mc:\\Users\\haora\\anaconda3\\envs\\graph_ml_course\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\School\\DELFT\\Graph_ML_project\\model.py:98\u001b[0m, in \u001b[0;36mDiscriminator.forward\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m     96\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodule_list[i](x\u001b[39m=\u001b[39mx , edge_index\u001b[39m=\u001b[39mdata\u001b[39m.\u001b[39medge_index)\n\u001b[0;32m     97\u001b[0m     x \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mDropout(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout_rate, inplace\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)(x)\n\u001b[1;32m---> 98\u001b[0m     x \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39;49mPReLU()(x)\n\u001b[0;32m    100\u001b[0m \u001b[39m# Pool graph node features\u001b[39;00m\n\u001b[0;32m    101\u001b[0m x \u001b[39m=\u001b[39m global_add_pool(x, data\u001b[39m.\u001b[39mbatch)\n",
      "File \u001b[1;32mc:\\Users\\haora\\anaconda3\\envs\\graph_ml_course\\lib\\site-packages\\torch\\nn\\modules\\activation.py:1329\u001b[0m, in \u001b[0;36mPReLU.__init__\u001b[1;34m(self, num_parameters, init, device, dtype)\u001b[0m\n\u001b[0;32m   1327\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_parameters \u001b[39m=\u001b[39m num_parameters\n\u001b[0;32m   1328\u001b[0m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n\u001b[1;32m-> 1329\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight \u001b[39m=\u001b[39m Parameter(torch\u001b[39m.\u001b[39;49mempty(num_parameters, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfactory_kwargs)\u001b[39m.\u001b[39;49mfill_(init))\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "path = r'C:\\School\\DELFT\\Graph_ML_project\\data\\swiss-dwellings-v3.0.0'\n",
    "dataset = Transformed_PolyGraphDataset(path)\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "n_batches = len(dataloader)\n",
    "print(n_batches)\n",
    "\n",
    "losses_g, losses_d = train(generator, discriminator, optimizer_G, optimizer_D, dataloader, max_epochs=10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing model for batches of input and gradient penalty for batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(edge_index=[2, 72], geometry=[42, 60], category=[42, 13], num_nodes=42)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataBatch(edge_index=[2, 2304], geometry=[1344, 60], category=[1344, 13], num_nodes=1344, batch=[1344], ptr=[33])\n",
      "noise:  torch.Size([1344, 128])\n",
      "fake:  torch.Size([1344, 60])\n",
      "real:  torch.Size([1344, 60])\n",
      "fake:  DataBatch(edge_index=[2, 2304], geometry=[1344, 60], batch=[1344])\n",
      "real:  DataBatch(edge_index=[2, 2304], geometry=[1344, 60], category=[1344, 13], num_nodes=1344, batch=[1344], ptr=[33])\n",
      "discriminator score fake:  torch.Size([32, 1])\n",
      "discriminator score real:  torch.Size([32, 1])\n",
      "tensor(0.5717, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "from model import gradient_penalty\n",
    "\n",
    "print(dataset[0])\n",
    "test_list = [dataset[0] for _ in range(32)]\n",
    "test_batch = Batch.from_data_list(test_list)\n",
    "print(test_batch)\n",
    "\n",
    "real = test_batch\n",
    "\n",
    "noise = th.randn((len(real.category), NOISE_SIZE))\n",
    "print(\"noise: \", noise.shape)\n",
    "fake = generator(real, noise)     \n",
    "\n",
    "# fake.shape = (batch_size * nodes, output_features = 60)\n",
    "# We must turn this into appropriate (batch) input for the discriminator\n",
    "fake = Batch(geometry=fake, edge_index=real.edge_index, batch=real.batch)\n",
    "\n",
    "print(\"fake: \", fake.geometry.shape)\n",
    "print(\"real: \", real.geometry.shape)\n",
    "\n",
    "discriminator_fake = discriminator(fake)    # discriminator scores for fakes\n",
    "discriminator_real = discriminator(real)    # discriminator scores for reals\n",
    "\n",
    "print(\"fake: \", fake)\n",
    "print(\"real: \", real)\n",
    "print(\"discriminator score fake: \", discriminator_fake.shape)\n",
    "print(\"discriminator score real: \", discriminator_real.shape)\n",
    "\n",
    "gp = gradient_penalty(discriminator, real, fake)\n",
    "print(gp)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing dataloader iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "dataset = Transformed_PolyGraphDataset(path)\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "dataiter = iter(dataloader)\n",
    "\n",
    "for _ in tqdm(range(n_batches)):\n",
    "    batch = next(dataiter)\n",
    "    print(batch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graph_ml_course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
